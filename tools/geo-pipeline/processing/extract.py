"""
Geo Pipeline - Extract Module
æå–æ¨¡å—ï¼šæå–æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰

èŒè´£ï¼š
- æå–Markdownæ ‡é¢˜
- æå–åˆ—è¡¨é¡¹
- ç”Ÿæˆç»“æ„åŒ–metadata
- è¾“å‡ºunit JSON
"""

import json
import re
from pathlib import Path


def extract_headings(text, max_level=3):
    """
    æå–Markdownæ ‡é¢˜

    Args:
        text: Markdownæ–‡æœ¬
        max_level: æœ€å¤§æ ‡é¢˜å±‚çº§ï¼ˆ1-6ï¼‰

    Returns:
        æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« level å’Œ text
    """
    headings = []

    for line in text.split('\n'):
        # åŒ¹é… Markdown æ ‡é¢˜æ ¼å¼ï¼š# Title, ## Title, etc.
        match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
        if match:
            level = len(match.group(1))
            if level <= max_level:
                title = match.group(2).strip()
                # ç§»é™¤å¯èƒ½çš„æ ‡é¢˜æœ«å°¾çš„ #
                title = re.sub(r'\s*#+\s*$', '', title)
                headings.append({
                    'level': level,
                    'text': title
                })

    return headings


def extract_bullets(text):
    """
    æå–åˆ—è¡¨é¡¹

    Args:
        text: Markdownæ–‡æœ¬

    Returns:
        åˆ—è¡¨é¡¹å†…å®¹åˆ—è¡¨
    """
    bullets = []

    for line in text.split('\n'):
        # åŒ¹é…æ— åºåˆ—è¡¨ï¼š- item, * item, + item
        match = re.match(r'^\s*[-*+]\s+(.+)$', line)
        if match:
            bullet_text = match.group(1).strip()
            bullets.append(bullet_text)

    return bullets


def extract_structure(config, dry_run=False):
    """
    æå–æ–‡æ¡£ç»“æ„

    Args:
        config: é…ç½®å­—å…¸
        dry_run: æ˜¯å¦å¹²è¿è¡Œ

    Returns:
        æå–ç»Ÿè®¡ä¿¡æ¯
    """
    print("ğŸ“‹ Extract: æå–æ ‡é¢˜ã€åˆ—è¡¨ç­‰ç»“æ„")

    input_dir = config['paths']['processed'] / 'chunks'
    output_dir = config['paths']['processed'] / 'units'

    # è·å–æ‰€æœ‰chunkæ–‡ä»¶
    chunk_files = list(input_dir.rglob('*_chunks.json'))
    print(f"   Found {len(chunk_files)} chunk files")

    if len(chunk_files) == 0:
        print("   âš ï¸  No chunk files to process")
        return {"files_found": 0, "units_created": 0}

    if dry_run:
        print("   [DRY RUN] Would extract structure")
        print(f"   Output directory: {output_dir}")
        print(f"   Max heading level: {config['processing']['max_heading_level']}")
        # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
        for f in chunk_files[:5]:
            rel_path = f.relative_to(input_dir)
            print(f"     - {rel_path}")
        if len(chunk_files) > 5:
            print(f"     ... and {len(chunk_files) - 5} more files")
        return {
            "files_found": len(chunk_files),
            "units_created": 0
        }

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç»Ÿè®¡
    unit_id = 0
    processed_files = 0
    failed_files = 0
    units_with_headings = 0
    units_with_bullets = 0

    # å¤„ç†æ¯ä¸ªchunkæ–‡ä»¶
    for i, chunk_file in enumerate(chunk_files, 1):
        try:
            # è¯»å–chunkæ•°æ®
            data = json.loads(chunk_file.read_text(encoding='utf-8'))

            # å¤„ç†æ¯ä¸ªchunk
            for chunk_idx, chunk in enumerate(data['chunks']):
                # æå–ç»“æ„
                headings = extract_headings(
                    chunk['content'],
                    max_level=config['processing']['max_heading_level']
                )
                bullets = extract_bullets(chunk['content'])

                # åˆ›å»ºunit
                unit = {
                    'id': f"unit_{unit_id:06d}",
                    'source_file': data['source_file'],
                    'chunk_index': chunk_idx,
                    'content': chunk['content'],
                    'metadata': {
                        'headings': headings,
                        'bullets': bullets,
                        'char_count': chunk['char_count']
                    },
                    # v0.2é¢„ç•™å­—æ®µ
                    'embeddings': None,
                    'entities': None,
                    'claims': None
                }

                # ä¿å­˜unit
                unit_file = output_dir / f"unit_{unit_id:06d}.json"
                unit_file.write_text(
                    json.dumps(unit, indent=2, ensure_ascii=False),
                    encoding='utf-8'
                )

                # ç»Ÿè®¡
                if headings:
                    units_with_headings += 1
                if bullets:
                    units_with_bullets += 1

                unit_id += 1

            processed_files += 1

            if i <= 10 or i % 100 == 0:
                rel_path = chunk_file.relative_to(input_dir)
                print(f"   [{i}/{len(chunk_files)}] âœ… {rel_path} â†’ {len(data['chunks'])} units")

        except Exception as e:
            failed_files += 1
            if i <= 10 or i % 100 == 0:
                print(f"   [{i}/{len(chunk_files)}] âŒ {chunk_file.name}: {e}")

    # æ€»ç»“
    print(f"\n   âœ… Extraction complete:")
    print(f"      Chunk files found: {len(chunk_files)}")
    print(f"      Processed: {processed_files}")
    print(f"      Failed: {failed_files}")
    print(f"      Total units created: {unit_id}")
    print(f"      Units with headings: {units_with_headings}")
    print(f"      Units with bullets: {units_with_bullets}")

    return {
        "files_found": len(chunk_files),
        "files_processed": processed_files,
        "files_failed": failed_files,
        "units_created": unit_id,
        "units_with_headings": units_with_headings,
        "units_with_bullets": units_with_bullets
    }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from pathlib import Path

    print("Testing extract module...")

    # æµ‹è¯•1: extract_headingså‡½æ•°
    print("\n1. Testing extract_headings function:")
    test_markdown = """
# ä¸€çº§æ ‡é¢˜
è¿™æ˜¯ä¸€äº›å†…å®¹ã€‚

## äºŒçº§æ ‡é¢˜
æ›´å¤šå†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜
- åˆ—è¡¨é¡¹1
- åˆ—è¡¨é¡¹2

#### å››çº§æ ‡é¢˜ï¼ˆåº”è¯¥è¢«è¿‡æ»¤ï¼‰
"""
    headings = extract_headings(test_markdown, max_level=3)
    print(f"   Found {len(headings)} headings:")
    for h in headings:
        print(f"   - Level {h['level']}: {h['text']}")

    # æµ‹è¯•2: extract_bulletså‡½æ•°
    print("\n2. Testing extract_bullets function:")
    bullets = extract_bullets(test_markdown)
    print(f"   Found {len(bullets)} bullets:")
    for b in bullets:
        print(f"   - {b}")

    # æµ‹è¯•3: extract_structureå‡½æ•°ï¼ˆdry runï¼‰
    print("\n3. Testing extract_structure function (dry run):")
    test_config = {
        'paths': {
            'processed': Path.home() / 'data/processed/shengcai'
        },
        'processing': {
            'max_heading_level': 3
        }
    }
    extract_structure(test_config, dry_run=True)
