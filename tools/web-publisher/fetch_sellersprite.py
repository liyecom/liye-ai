#!/usr/bin/env python3
"""
ä»Žå–å®¶ç²¾çµåšå®¢æŠ“å–é«˜è´¨é‡ Amazon å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
import re


def fetch_article_list(start_index=0):
    """èŽ·å–æ–‡ç« åˆ—è¡¨"""
    url = f"https://www.sellersprite.com/cn/blog?startIndex={start_index}"

    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        articles = []
        # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« ï¼ˆä½¿ç”¨ class="article"ï¼‰
        for article in soup.find_all(class_='article'):
            # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æŽ¥
            title_elem = article.find(class_='article-title')
            link_elem = article.find('a') if article.find('a') else None

            if title_elem and link_elem:
                href = link_elem.get('href', '')
                # ç¡®ä¿æ˜¯å®Œæ•´ URL
                if href.startswith('/'):
                    href = f"https://www.sellersprite.com{href}"
                elif not href.startswith('http'):
                    continue  # è·³è¿‡æ— æ•ˆé“¾æŽ¥

                articles.append({
                    'title': title_elem.get_text(strip=True),
                    'url': href
                })

        return articles

    except Exception as e:
        print(f"âŒ èŽ·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        return []


def fetch_article_content(url):
    """èŽ·å–å•ç¯‡æ–‡ç« å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # æ‰¾åˆ°æ–‡ç« ä¸»ä½“ï¼ˆæ ¹æ®å–å®¶ç²¾çµç½‘ç«™ç»“æž„è°ƒæ•´ï¼‰
        article_body = soup.find('article') or soup.find('div', class_='entry-content') or soup.find('main')

        if not article_body:
            print(f"   âš ï¸  æ— æ³•æ‰¾åˆ°æ–‡ç« ä¸»ä½“")
            return None

        # æå–æ ‡é¢˜
        title = soup.find('h1')
        title_text = title.get_text(strip=True) if title else "æœªçŸ¥æ ‡é¢˜"

        # è½¬æ¢ä¸º Markdown
        markdown = f"# {title_text}\\n\\n"

        # æå–æ®µè½å’Œæ ‡é¢˜
        for elem in article_body.find_all(['p', 'h2', 'h3', 'h4', 'ul', 'ol']):
            if elem.name == 'p':
                text = elem.get_text(strip=True)
                if text and len(text) > 10:
                    markdown += f"{text}\\n\\n"
            elif elem.name == 'h2':
                markdown += f"## {elem.get_text(strip=True)}\\n\\n"
            elif elem.name == 'h3':
                markdown += f"### {elem.get_text(strip=True)}\\n\\n"
            elif elem.name == 'h4':
                markdown += f"#### {elem.get_text(strip=True)}\\n\\n"
            elif elem.name in ['ul', 'ol']:
                for li in elem.find_all('li'):
                    markdown += f"- {li.get_text(strip=True)}\\n"
                markdown += "\\n"

        # æ£€æŸ¥å†…å®¹é•¿åº¦
        char_count = len(markdown)
        if char_count < 1000:
            print(f"   âš ï¸  å†…å®¹å¤ªçŸ­ï¼ˆ{char_count} å­—ç¬¦ï¼‰ï¼Œè·³è¿‡")
            return None

        return {
            'title': title_text,
            'content': markdown,
            'url': url,
            'char_count': char_count
        }

    except Exception as e:
        print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
        return None


def save_to_markdown(article, output_dir):
    """ä¿å­˜ä¸º Markdown æ–‡ä»¶"""
    # ç”Ÿæˆæ–‡ä»¶å
    filename = re.sub(r'[^\w\s-]', '', article['title'])
    filename = re.sub(r'[-\s]+', '-', filename)
    filename = f"{filename[:50]}.md"

    filepath = output_dir / filename

    # æ·»åŠ å…ƒä¿¡æ¯
    content = f"""---
source: å–å®¶ç²¾çµ
source_url: {article['url']}
fetched_at: {time.strftime('%Y-%m-%d')}
char_count: {article['char_count']}
---

{article['content']}

---

**æ¥æº**: [å–å®¶ç²¾çµ]({article['url']})
"""

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def main():
    print("ðŸš€ å¼€å§‹æŠ“å–å–å®¶ç²¾çµåšå®¢å†…å®¹...\\n")

    output_dir = Path.home() / 'github/liye_os/tools/web-publisher/fetched_articles'
    output_dir.mkdir(parents=True, exist_ok=True)

    # æµ‹è¯•ï¼šæŠ“å–ç¬¬ä¸€é¡µçš„å‰ 10 ç¯‡æ–‡ç« 
    print("ðŸ“‹ èŽ·å–æ–‡ç« åˆ—è¡¨...")
    articles = fetch_article_list(start_index=0)

    if not articles:
        print("âŒ æœªæ‰¾åˆ°æ–‡ç« ")
        return

    print(f"âœ… æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« \\n")

    # é™åˆ¶ä¸ºå‰ 10 ç¯‡ï¼ˆæµ‹è¯•ï¼‰
    articles = articles[:10]

    success = 0
    failed = 0

    for i, article in enumerate(articles, 1):
        print(f"[{i}/10] {article['title']}")

        # æŠ“å–æ–‡ç« å†…å®¹
        content = fetch_article_content(article['url'])

        if content:
            # ä¿å­˜æ–‡ä»¶
            filepath = save_to_markdown(content, output_dir)
            print(f"   âœ… å·²ä¿å­˜: {filepath.name} ({content['char_count']} å­—ç¬¦)")
            success += 1
        else:
            failed += 1

        # ç¤¼è²Œå»¶è¿Ÿ
        time.sleep(2)

    print(f"\\nâœ… æŠ“å–å®Œæˆ")
    print(f"   æˆåŠŸ: {success}")
    print(f"   å¤±è´¥: {failed}")
    print(f"   ä¿å­˜ä½ç½®: {output_dir}")


if __name__ == '__main__':
    main()
