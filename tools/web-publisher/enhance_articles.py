#!/usr/bin/env python3
"""
AI æ–‡ç« å¢å¼ºè„šæœ¬ - å°†æ™®é€šæ–‡ç« æå‡ä¸º 10x è´¨é‡
ä»¥"äºšé©¬é€Šé€‰å“å®æˆ˜"æ–‡ç« ä¸ºæ ‡å‡†æ¨¡æ¿

æ–¹æ¡ˆ Dï¼ˆæ··åˆç­–ç•¥ï¼‰- å·²å®æ–½ï¼š
1. æ¨¡å‹ï¼šClaude Sonnet 4.5 (claude-sonnet-4-5-20250929)
2. Max Tokens: 24,000ï¼ˆç¡®ä¿å®Œæ•´ç”Ÿæˆï¼‰
3. Temperature: 0.5ï¼ˆé™ä½å¹»è§‰ç‡ï¼‰
4. å®Œæ•´æ€§æ£€æµ‹ï¼šæ£€æŸ¥æ‰¿è¯ºçš„ç« èŠ‚æ˜¯å¦å…¨éƒ¨ç”Ÿæˆ
5. å¹»è§‰æ£€æµ‹ï¼šæ£€æµ‹ä¸­è‹±æ–‡æ··æ‚ã€ä¸å®Œæ•´è¡¨æ ¼
6. è‡ªåŠ¨æ ‡è®°ï¼šä¸¥é‡é—®é¢˜æ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æ ¸

é¢„æœŸæ•ˆæœï¼š
- å®Œæ•´ç‡ï¼š98%
- å¹»è§‰ç‡ï¼š<1%
- æˆæœ¬ï¼š~$0.95/ç¯‡
"""

import os
import sys
import json
import re
from pathlib import Path
from datetime import datetime

# æ£€æŸ¥ä¾èµ–
try:
    from anthropic import Anthropic
except ImportError:
    print("âŒ é”™è¯¯ï¼šæœªå®‰è£… anthropic åŒ…")
    print("è¯·è¿è¡Œ: pip install anthropic")
    sys.exit(1)

try:
    from dotenv import load_dotenv
    # åŠ è½½ .env æ–‡ä»¶
    env_path = Path.home() / "github/liye_os/.env"
    if env_path.exists():
        load_dotenv(env_path)
except ImportError:
    pass  # python-dotenv æœªå®‰è£…ï¼Œè·³è¿‡

# ================================
# é…ç½®
# ================================

API_KEY = os.getenv('ANTHROPIC_API_KEY')
if not API_KEY:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·è¿è¡Œ: export ANTHROPIC_API_KEY='your-api-key'")
    sys.exit(1)

client = Anthropic(api_key=API_KEY)

# è·¯å¾„é…ç½®
POSTS_DIR = Path("/Users/liye/github/liye_os/websites/amazon-optimization/src/content/posts")
OUTPUT_DIR = POSTS_DIR / "_enhanced"
OUTPUT_DIR.mkdir(exist_ok=True)

# ================================
# æ ‡å‡†æ–‡ç« æ¨¡æ¿ï¼ˆPromptï¼‰
# ================================

ENHANCEMENT_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äºšé©¬é€Šè¿è¥å†…å®¹ç¼–è¾‘ï¼Œæ“…é•¿å°†æŠ€æœ¯æ€§æ–‡ç« è½¬åŒ–ä¸ºå¸å¼•äººçš„å®æˆ˜æ•™ç¨‹ã€‚

# ä»»åŠ¡
å°†ä¸‹é¢çš„åŸå§‹æ–‡ç« æ”¹å†™ä¸º 10x è´¨é‡çš„æ·±åº¦æ•™ç¨‹ï¼Œå‚è€ƒä»¥ä¸‹æ ‡å‡†ï¼š

# è´¨é‡æ ‡å‡†ï¼ˆå‚è€ƒæ–‡ç« ï¼š"äºšé©¬é€Šé€‰å“å®æˆ˜ï¼šç«å“åº—é“ºæ³•30å¤©æ‰¾åˆ°æœˆåˆ©æ¶¦5000ç¾å…ƒäº§å“"ï¼‰

## 1. ç»“æ„è¦æ±‚
- **å¼€å¤´**ï¼šæ ¸å¿ƒæ•°æ®è¡¨æ ¼ï¼ˆæŠ•èµ„å›æŠ¥ç‡ã€æ—¶é—´ã€åˆ©æ¶¦ç­‰ï¼‰
- **ç›®å½•**ï¼š6-8 ä¸ªç« èŠ‚ï¼Œå¯ç›´æ¥è·³è½¬ï¼ˆä½¿ç”¨ `<h2 id="ç« èŠ‚å">` æ ¼å¼ï¼‰
- **ç« èŠ‚åˆ†å¸ƒ**ï¼š
  - ç¬¬ä¸€éƒ¨åˆ†ï¼šç—›ç‚¹/é—®é¢˜ï¼ˆä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ–¹æ³•ï¼‰
  - ç¬¬äºŒéƒ¨åˆ†ï¼šå®Œæ•´æµç¨‹ï¼ˆ5-7 ä¸ªæ­¥éª¤ï¼‰
  - ç¬¬ä¸‰éƒ¨åˆ†ï¼šçœŸå®æ¡ˆä¾‹ï¼ˆæ•°æ® + æˆªå›¾æè¿°ï¼‰
  - ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®å¤ç›˜ï¼ˆæ—¶é—´çº¿ + è¡¨æ ¼ï¼‰
  - ç¬¬äº”éƒ¨åˆ†ï¼šé¿å‘æŒ‡å—ï¼ˆ3-5 ä¸ªå¸¸è§é”™è¯¯ï¼‰
  - ç¬¬å…­éƒ¨åˆ†ï¼šå·¥å…·æ¸…å•ï¼ˆå¿…å¤‡ + å…è´¹æ›¿ä»£ï¼‰

## 2. å†…å®¹è¦æ±‚
- **çœŸå®æ•°æ®**ï¼šå…·ä½“æ•°å­—ï¼ˆä¸è¦"å¤§çº¦"ã€"å¾ˆå¤š"ï¼Œè¦"187 å•"ã€"$4,847"ï¼‰
- **å¯æ“ä½œæ€§**ï¼šæ‰‹æŠŠæ‰‹æ•™å­¦ï¼ˆ"ç‚¹å‡»å“ªé‡Œ"ã€"è¾“å…¥ä»€ä¹ˆ"ã€"å¦‚ä½•ç­›é€‰"ï¼‰
- **è¡¨æ ¼å¯è§†åŒ–**ï¼šæ•°æ®ç”¨è¡¨æ ¼å‘ˆç°ï¼ˆMarkdown è¡¨æ ¼ï¼‰
- **å¯¹æ¯”åˆ†æ**ï¼šå¤±è´¥æ¡ˆä¾‹ vs æˆåŠŸæ¡ˆä¾‹
- **æ—¶é—´çº¿**ï¼šWeek 1-8 è¯¦ç»†è®°å½•
- **å·¥å…·æ¨è**ï¼šå…·ä½“å·¥å…·å + ä»·æ ¼ + åŠŸèƒ½

## 3. å†™ä½œé£æ ¼
- **ç¬¬ä¸€äººç§°**ï¼š"æˆ‘çš„å¤±è´¥å²"ã€"æˆ‘å¦‚ä½•æ“ä½œ"
- **å¯¹è¯æ„Ÿ**ï¼šæé—® + å›ç­”ï¼ˆ"ä¸ºä»€ä¹ˆï¼Ÿå› ä¸º..."ï¼‰
- **ç—›ç‚¹å…ˆè¡Œ**ï¼šå…ˆè®²å¤±è´¥ï¼Œå†è®²æˆåŠŸ
- **æ•°æ®æ”¯æ’‘**ï¼šæ¯ä¸ªç»“è®ºéƒ½æœ‰æ•°æ®
- **å¯å¤åˆ¶æ€§**ï¼šè¯»è€…çœ‹å®Œèƒ½ç«‹å³æ‰§è¡Œ

## 4. Frontmatter è¦æ±‚
ç”Ÿæˆå®Œæ•´çš„ frontmatterï¼ˆæ³¨æ„ï¼šä¸è¦ç”¨ ```yaml åŒ…è£¹ï¼Œç›´æ¥è¾“å‡ºï¼‰ï¼š
---
title: "æ ‡é¢˜ï¼ˆå¸å¼•ç‚¹å‡»ï¼Œ50 å­—ç¬¦ä»¥å†…ï¼‰"
description: "æè¿°ï¼ˆSEO ä¼˜åŒ–ï¼Œ150 å­—ç¬¦ä»¥å†…ï¼ŒåŒ…å«å…³é”®è¯ï¼‰"
pubDate: 2025-12-27
category: "äºšé©¬é€Šè¿è¥"
keywords: ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", "å…³é”®è¯4", "å…³é”®è¯5"]
intent: "commercial"  # informational/commercial/transactional
---

## 5. å¿…é¡»åŒ…å«çš„å…ƒç´ 
- [ ] å¼€ç¯‡æ•°æ®è¡¨æ ¼ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
- [ ] ç›®å½•ï¼ˆ6-8 ä¸ªç« èŠ‚ï¼Œå¸¦é”šç‚¹ï¼‰
- [ ] è‡³å°‘ 3 ä¸ªæ•°æ®è¡¨æ ¼
- [ ] è‡³å°‘ 1 ä¸ªå¤±è´¥æ¡ˆä¾‹
- [ ] è‡³å°‘ 1 ä¸ªæˆåŠŸæ¡ˆä¾‹
- [ ] å…·ä½“å·¥å…·æ¨èï¼ˆåç§° + ä»·æ ¼ï¼‰
- [ ] å¯å¤åˆ¶çš„æµç¨‹ï¼ˆæ­¥éª¤ 1-Nï¼‰
- [ ] é¿å‘æŒ‡å—ï¼ˆå¸¸è§é”™è¯¯ï¼‰

---

# åŸå§‹æ–‡ç« 

{original_content}

---

# è¾“å‡ºè¦æ±‚

1. **è¾“å‡ºå®Œæ•´çš„ Markdown æ–‡ä»¶**ï¼ˆåŒ…å« frontmatterï¼‰
2. **å­—æ•°è¦æ±‚**ï¼š6,000-10,000 å­—
3. **ç« èŠ‚é”šç‚¹**ï¼šä½¿ç”¨ `<h2 id="ç« èŠ‚å">` æ ¼å¼ï¼ˆæ–¹ä¾¿ç›®å½•è·³è½¬ï¼‰
4. **è¡¨æ ¼**ï¼šè‡³å°‘ 5 ä¸ªæ•°æ®è¡¨æ ¼
5. **å¯æ“ä½œæ€§**ï¼šè¯»è€…çœ‹å®Œèƒ½ç«‹å³æ‰§è¡Œ

# ç‰¹åˆ«æé†’

- ä¸è¦ä½¿ç”¨"æ ¹æ®æ–‡ç« å†…å®¹"ã€"åŸæ–‡æåˆ°"ç­‰å…ƒè¯­è¨€
- ä¸è¦å†™"æœ¬æ–‡å°†ä»‹ç»"ï¼Œç›´æ¥å¼€å§‹
- ä¸è¦ç”¨"æˆ‘ä»¬"ï¼Œç”¨"æˆ‘"
- æ•°æ®è¦å…·ä½“ï¼ˆä¸è¦"å¾ˆå¤š"ï¼Œè¦"187 å•"ï¼‰
- é¿å…ç©ºæ´çš„å»ºè®®ï¼ˆå¦‚"è®¤çœŸåˆ†æ"ï¼‰ï¼Œè¦å…·ä½“æ“ä½œï¼ˆå¦‚"æ‰“å¼€ Jungle Scoutï¼Œç‚¹å‡» Product Databaseï¼Œè®¾ç½®ç­›é€‰æ¡ä»¶ï¼šæœˆé”€é‡ 300-1000"ï¼‰

ç°åœ¨å¼€å§‹æ”¹å†™ï¼š
"""

# ================================
# å·¥å…·å‡½æ•°
# ================================

def should_skip(file_path):
    """åˆ¤æ–­æ˜¯å¦è·³è¿‡æ–‡ä»¶"""
    name = file_path.name

    # è·³è¿‡ README æ–‡ä»¶
    if 'README' in name or name.startswith('_'):
        return True, "README æ–‡ä»¶"

    # è·³è¿‡æ ‡å‡†æ–‡ç« ï¼ˆæ¨¡æ¿ï¼‰
    if 'äºšé©¬é€Šé€‰å“å®æˆ˜ç«å“åº—é“ºæ³•' in name:
        return True, "æ ‡å‡†æ¨¡æ¿æ–‡ç« "

    return False, None

def clean_frontmatter(content):
    """æ¸…ç†å’ŒéªŒè¯ frontmatter æ ¼å¼"""
    # ç§»é™¤å¯èƒ½çš„ ```yaml åŒ…è£¹
    if content.startswith('```yaml\n'):
        content = content[8:]  # ç§»é™¤ ```yaml\n

    if content.startswith('```\n'):
        content = content[4:]  # ç§»é™¤ ```\n

    # ç¡®ä¿ä»¥ --- å¼€å¤´
    if not content.startswith('---\n'):
        content = '---\n' + content

    # ç§»é™¤å¯èƒ½çš„ç»“å°¾ ```
    content = re.sub(r'\n```\s*$', '', content)

    return content

def detect_completeness_issues(content):
    """æ£€æµ‹æ–‡ç« å®Œæ•´æ€§é—®é¢˜ï¼ˆæ–¹æ¡ˆ D ç¬¬ 4 æ­¥ï¼‰"""
    issues = []

    # æå–ç›®å½•ä¸­æ‰¿è¯ºçš„ç« èŠ‚
    toc_pattern = r'##\s+ç›®å½•\s*\n(.*?)(?=\n##|\Z)'
    toc_match = re.search(toc_pattern, content, re.DOTALL)

    if toc_match:
        toc_text = toc_match.group(1)
        # æå–ç›®å½•ä¸­çš„ç« èŠ‚ï¼ˆæ ¼å¼ï¼š- [ç« èŠ‚å](#anchor)ï¼‰
        promised_chapters = re.findall(r'-\s+\[([^\]]+)\]', toc_text)

        # æ£€æŸ¥æ¯ä¸ªç« èŠ‚æ˜¯å¦å­˜åœ¨
        for chapter in promised_chapters:
            # ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…ç« èŠ‚æ ‡é¢˜
            patterns = [
                rf'<h2 id="[^"]*">{re.escape(chapter)}</h2>',
                rf'##\s+{re.escape(chapter)}',
                rf'<h2[^>]*>{re.escape(chapter)}</h2>'
            ]

            found = any(re.search(p, content) for p in patterns)
            if not found:
                issues.append(f"ç¼ºå¤±ç« èŠ‚: {chapter}")

    # æ£€æŸ¥æ–‡ç« æ˜¯å¦çªç„¶ç»“æŸï¼ˆæœ«å°¾æ²¡æœ‰ç»“è®ºæ€§å†…å®¹ï¼‰
    last_500_chars = content[-500:]
    conclusion_markers = ['æ€»ç»“', 'ç»“è¯­', 'å°ç»“', 'æœ€å', 'æ€»ä¹‹', 'ç»¼ä¸Šæ‰€è¿°']
    has_conclusion = any(marker in last_500_chars for marker in conclusion_markers)

    if not has_conclusion and len(content) > 5000:
        issues.append("æ–‡ç« å¯èƒ½æœªå®Œæˆï¼ˆç¼ºå°‘ç»“è®ºæ€§å†…å®¹ï¼‰")

    return issues

def detect_hallucination(content):
    """æ£€æµ‹å¹»è§‰é—®é¢˜ï¼ˆæ–¹æ¡ˆ D ç¬¬ 5 æ­¥ï¼‰- ä¸­è‹±æ–‡æ··æ‚"""
    issues = []

    # æ£€æµ‹ä¸­è‹±æ–‡æ··æ‚æ¨¡å¼ï¼ˆä¸­æ–‡è¯æ±‡ä¸­å¤¹æ‚è‹±æ–‡å­—æ¯ï¼‰
    # ä¾‹å¦‚ï¼š"äººå·¥å¹²prejection"ã€"æ•°æ®analyzåˆ†æ"
    hallucination_patterns = [
        # ä¸­æ–‡ + è‹±æ–‡ + ä¸­æ–‡ï¼ˆå¯ç–‘ï¼‰
        r'[\u4e00-\u9fa5]{1,}[a-zA-Z]{3,}[\u4e00-\u9fa5]{1,}',
        # ä¸­æ–‡è¯ç»„ä¸­é—´æ’å…¥è‹±æ–‡ï¼ˆéå¸¸å¯ç–‘ï¼‰
        r'[\u4e00-\u9fa5][a-zA-Z]{2,}[\u4e00-\u9fa5]'
    ]

    suspicious_texts = []
    for pattern in hallucination_patterns:
        matches = re.finditer(pattern, content)
        for match in matches:
            text = match.group()
            # æ’é™¤å¸¸è§çš„æ­£å¸¸æƒ…å†µï¼ˆå¦‚ "A/Bæµ‹è¯•"ã€"SEOä¼˜åŒ–"ï¼‰
            if not re.match(r'[\u4e00-\u9fa5]{0,2}[A-Z]{1,3}/?[A-Z]{0,3}[\u4e00-\u9fa5]{0,2}', text):
                suspicious_texts.append(text)

    if suspicious_texts:
        # å»é‡å¹¶é™åˆ¶æ˜¾ç¤ºå‰ 5 ä¸ª
        unique_texts = list(set(suspicious_texts))[:5]
        issues.append(f"æ£€æµ‹åˆ°ä¸­è‹±æ–‡æ··æ‚ï¼ˆç–‘ä¼¼å¹»è§‰ï¼‰: {', '.join(unique_texts)}")

    # æ£€æµ‹ä¸å®Œæ•´çš„ Markdown è¡¨æ ¼
    table_lines = [line for line in content.split('\n') if line.strip().startswith('|')]
    if table_lines:
        for i, line in enumerate(table_lines):
            cells = [c.strip() for c in line.split('|')]
            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå•å…ƒæ ¼æˆ–å¼‚å¸¸çŸ­çš„å•å…ƒæ ¼
            empty_cells = sum(1 for c in cells if len(c) < 2)
            if empty_cells > len(cells) * 0.3:  # è¶…è¿‡ 30% çš„å•å…ƒæ ¼ä¸ºç©º
                issues.append(f"è¡¨æ ¼ç¬¬ {i+1} è¡Œæ•°æ®ä¸å®Œæ•´")
                break  # åªæŠ¥å‘Šç¬¬ä¸€ä¸ªé—®é¢˜

    return issues

def validate_article(content):
    """éªŒè¯æ–‡ç« è´¨é‡ï¼ˆæ•´åˆæ–¹æ¡ˆ D çš„æ£€æµ‹ï¼‰"""
    issues = []

    # æ£€æŸ¥ frontmatter
    if not content.startswith('---\n'):
        issues.append("ç¼ºå°‘ frontmatter å¼€å¤´æ ‡è®°")

    # æ£€æŸ¥å¿…é¡»å­—æ®µ
    required_fields = ['title:', 'description:', 'pubDate:', 'category:', 'keywords:', 'intent:']
    for field in required_fields:
        if field not in content[:500]:  # å‰ 500 å­—ç¬¦å†…åº”è¯¥åŒ…å«
            issues.append(f"ç¼ºå°‘å¿…é¡»å­—æ®µ: {field}")

    # æ£€æŸ¥æ–‡ç« é•¿åº¦
    if len(content) < 5000:
        issues.append(f"æ–‡ç« å¤ªçŸ­: {len(content)} å­—ç¬¦ï¼ˆå»ºè®® > 6000ï¼‰")

    # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨æ ¼
    if content.count('|') < 10:  # è‡³å°‘åº”è¯¥æœ‰å‡ ä¸ªè¡¨æ ¼
        issues.append("è¡¨æ ¼æ•°é‡ä¸è¶³")

    # æ£€æŸ¥æ˜¯å¦æœ‰ç« èŠ‚é”šç‚¹
    if '<h2 id=' not in content:
        issues.append("ç¼ºå°‘ç« èŠ‚é”šç‚¹ï¼ˆ<h2 id=ï¼‰")

    # æ–¹æ¡ˆ Dï¼šå®Œæ•´æ€§æ£€æµ‹
    completeness_issues = detect_completeness_issues(content)
    issues.extend(completeness_issues)

    # æ–¹æ¡ˆ Dï¼šå¹»è§‰æ£€æµ‹
    hallucination_issues = detect_hallucination(content)
    issues.extend(hallucination_issues)

    return issues

def enhance_article(file_path):
    """å¢å¼ºå•ç¯‡æ–‡ç« """
    print(f"\n{'='*60}")
    print(f"ğŸ“„ å¤„ç†: {file_path.name}")
    print(f"{'='*60}")

    # æ£€æŸ¥æ˜¯å¦è·³è¿‡
    skip, reason = should_skip(file_path)
    if skip:
        print(f"â­ï¸  è·³è¿‡: {reason}")
        return None

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼‰
    output_path = OUTPUT_DIR / file_path.name
    if output_path.exists():
        print(f"â­ï¸  è·³è¿‡: å·²å¤„ç†ï¼ˆè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼‰")
        return None

    # è¯»å–åŸæ–‡
    print("ğŸ“– è¯»å–åŸæ–‡...")
    with open(file_path, 'r', encoding='utf-8') as f:
        original_content = f.read()

    original_length = len(original_content)
    print(f"   åŸæ–‡é•¿åº¦: {original_length:,} å­—ç¬¦")

    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å¢å¼ºåçš„æ–‡ç« ï¼ˆé•¿åº¦ > 10000 å­—ç¬¦ï¼‰
    if original_length > 10000:
        print(f"â­ï¸  è·³è¿‡: ç–‘ä¼¼å·²å¢å¼ºæ–‡ç« ï¼ˆé•¿åº¦ > 10000ï¼‰")
        return None

    # è°ƒç”¨ Claude API
    try:
        print("ğŸ“¡ è°ƒç”¨ Claude APIï¼ˆæ–¹æ¡ˆ D - Sonnet 4.5ï¼ŒStreaming æ¨¡å¼ï¼‰...")

        # ä½¿ç”¨ streaming æ¨¡å¼å¤„ç†é•¿æ–‡æœ¬ç”Ÿæˆ
        enhanced_content = ""
        with client.messages.stream(
            model="claude-sonnet-4-5-20250929",  # æ–¹æ¡ˆ Dï¼šClaude Sonnet 4.5
            max_tokens=24000,  # æ–¹æ¡ˆ Dï¼šå¢åŠ åˆ° 24K
            temperature=0.5,   # æ–¹æ¡ˆ Dï¼šé™ä½æ¸©åº¦å‡å°‘å¹»è§‰
            messages=[{
                "role": "user",
                "content": ENHANCEMENT_PROMPT.format(original_content=original_content)
            }]
        ) as stream:
            for text in stream.text_stream:
                enhanced_content += text
                # æ˜¾ç¤ºè¿›åº¦
                if len(enhanced_content) % 1000 == 0:
                    print(f"   ç”Ÿæˆä¸­: {len(enhanced_content)} å­—ç¬¦...", end='\r')

            # è·å–æœ€ç»ˆæ¶ˆæ¯å¯¹è±¡ä»¥è·å– token ä½¿ç”¨æƒ…å†µ
            message = stream.get_final_message()

        print(f"   ç”Ÿæˆå®Œæˆ: {len(enhanced_content)} å­—ç¬¦     ")

        # æ¸…ç† frontmatter æ ¼å¼
        print("ğŸ”§ æ¸…ç† frontmatter...")
        enhanced_content = clean_frontmatter(enhanced_content)

        # éªŒè¯æ–‡ç« è´¨é‡ï¼ˆæ–¹æ¡ˆ Dï¼šåŒ…å«å®Œæ•´æ€§å’Œå¹»è§‰æ£€æµ‹ï¼‰
        print("ğŸ” éªŒè¯æ–‡ç« è´¨é‡ï¼ˆå®Œæ•´æ€§ + å¹»è§‰æ£€æµ‹ï¼‰...")
        issues = validate_article(enhanced_content)

        if issues:
            print("âš ï¸  è´¨é‡é—®é¢˜:")
            for issue in issues:
                print(f"   - {issue}")

            # æ–¹æ¡ˆ Dï¼šæ ‡è®°éœ€è¦äººå·¥å®¡æ ¸
            critical_issues = [i for i in issues if 'ç¼ºå¤±ç« èŠ‚' in i or 'å¹»è§‰' in i or 'æœªå®Œæˆ' in i]
            if critical_issues:
                print("ğŸš¨ ä¸¥é‡é—®é¢˜ï¼ˆéœ€è¦äººå·¥å®¡æ ¸ï¼‰:")
                for issue in critical_issues:
                    print(f"   - {issue}")
        else:
            print("âœ… è´¨é‡æ£€æŸ¥é€šè¿‡")

        # ç»Ÿè®¡ï¼ˆSonnet å®šä»·ï¼‰
        input_tokens = message.usage.input_tokens
        output_tokens = message.usage.output_tokens
        cost = input_tokens * 0.000003 + output_tokens * 0.000015  # Sonnet å®šä»·

        enhanced_length = len(enhanced_content)
        improvement_ratio = enhanced_length / original_length

        print(f"ğŸ“Š å¢å¼ºå®Œæˆ")
        print(f"   å¢å¼ºåé•¿åº¦: {enhanced_length:,} å­—ç¬¦")
        print(f"   æ”¹è¿›å€æ•°: {improvement_ratio:.1f}x")
        print(f"   è¾“å…¥ tokens: {input_tokens:,}")
        print(f"   è¾“å‡º tokens: {output_tokens:,}")
        print(f"   æˆæœ¬: ${cost:.4f}")

        # ä¿å­˜å¢å¼ºåçš„æ–‡ç« 
        output_path = OUTPUT_DIR / file_path.name
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(enhanced_content)

        print(f"ğŸ’¾ å·²ä¿å­˜: {output_path}")

        return {
            'file': file_path.name,
            'original_length': original_length,
            'enhanced_length': enhanced_length,
            'improvement_ratio': improvement_ratio,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'cost': cost,
            'output_path': str(output_path)
        }

    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    import sys

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    test_mode = '--test' in sys.argv
    batch_10 = '--batch10' in sys.argv
    yes_flag = '--yes' in sys.argv or '-y' in sys.argv

    print("=" * 60)
    print("ğŸš€ AI æ–‡ç« å¢å¼ºè„šæœ¬ - 10x è´¨é‡æå‡")
    print("=" * 60)
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {POSTS_DIR}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print()

    # è·å–æ‰€æœ‰æ–‡ç« 
    md_files = sorted(POSTS_DIR.glob("*.md"))
    total_files = len(md_files)

    print(f"ğŸ“Š æ‰¾åˆ° {total_files} ç¯‡æ–‡ç« ")
    print()

    # æ ¹æ®å‚æ•°é€‰æ‹©æ¨¡å¼
    if test_mode:
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰ 3 ç¯‡")
        md_files = md_files[:3]
    elif batch_10:
        print("ğŸ“¦ å°æ‰¹é‡æ¨¡å¼ï¼šåªå¤„ç†å‰ 10 ç¯‡")
        md_files = md_files[:10]
    else:
        # è¯¢é—®ç”¨æˆ·
        print("ğŸ¤” é€‰æ‹©æ¨¡å¼:")
        print("  1. æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰ 3 ç¯‡ï¼‰")
        print("  2. å°æ‰¹é‡ï¼ˆåªå¤„ç†å‰ 10 ç¯‡ï¼‰")
        print("  3. æ‰¹é‡æ¨¡å¼ï¼ˆå¤„ç†æ‰€æœ‰æ–‡ç« ï¼‰")
        choice = input("è¯·è¾“å…¥ (1/2/3): ").strip()

        if choice == '1':
            print("\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰ 3 ç¯‡")
            md_files = md_files[:3]
        elif choice == '2':
            print("\nğŸ“¦ å°æ‰¹é‡æ¨¡å¼ï¼šåªå¤„ç†å‰ 10 ç¯‡")
            md_files = md_files[:10]
        elif choice == '3':
            if not yes_flag:
                confirm = input(f"\nâš ï¸  ç¡®è®¤è¦å¤„ç† {total_files} ç¯‡æ–‡ç« å—ï¼Ÿ(y/n): ").strip().lower()
                if confirm != 'y':
                    print("âŒ å·²å–æ¶ˆ")
                    return
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return

    # é¢„ä¼°æˆæœ¬ï¼ˆæ–¹æ¡ˆ Dï¼šSonnet 4.5ï¼‰
    estimated_cost = len(md_files) * 0.95  # æ–¹æ¡ˆ Dï¼šæ¯ç¯‡çº¦ $0.95
    print(f"\nğŸ’° é¢„ä¼°æˆæœ¬: ${estimated_cost:.2f}")
    print(f"   (æ–¹æ¡ˆ D - Sonnet 4.5ï¼šé«˜è´¨é‡ï¼Œä½å¹»è§‰ç‡)")
    print()

    if not yes_flag:
        confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("âŒ å·²å–æ¶ˆ")
            return

    # å¤„ç†æ–‡ç« 
    results = []
    total_cost = 0
    skipped = 0

    start_time = datetime.now()

    for i, file_path in enumerate(md_files, 1):
        print(f"\n[{i}/{len(md_files)}]")
        result = enhance_article(file_path)

        if result:
            results.append(result)
            total_cost += result['cost']

            # é€Ÿç‡é™åˆ¶ï¼šæ¯åˆ†é’Ÿæœ€å¤š 10,000 output tokens
            # æ¯ç¯‡æ–‡ç« çº¦ 16,000 tokensï¼Œéœ€è¦ç­‰å¾… 2 åˆ†é’Ÿ
            if i < len(md_files):  # ä¸æ˜¯æœ€åä¸€ç¯‡
                print(f"\nâ³ ç­‰å¾… 120 ç§’é¿å…é€Ÿç‡é™åˆ¶...")
                import time
                time.sleep(120)
        else:
            skipped += 1

    # è®¡ç®—æ€»æ—¶é—´
    elapsed = datetime.now() - start_time

    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ: {len(results)} ç¯‡")
    print(f"â­ï¸  è·³è¿‡: {skipped} ç¯‡")
    print(f"ğŸ’° æ€»æˆæœ¬: ${total_cost:.2f}")
    print(f"â±ï¸  æ€»è€—æ—¶: {int(elapsed.total_seconds())} ç§’ ({elapsed.total_seconds()/60:.1f} åˆ†é’Ÿ)")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print()

    if results:
        avg_cost = total_cost / len(results)
        avg_improvement = sum(r['improvement_ratio'] for r in results) / len(results)

        print(f"ğŸ“ˆ å¹³å‡æ•°æ®:")
        print(f"   å¹³å‡æˆæœ¬: ${avg_cost:.4f}/ç¯‡")
        print(f"   å¹³å‡æ”¹è¿›: {avg_improvement:.1f}x")
        print()

    # ä¿å­˜ç»Ÿè®¡
    stats_file = OUTPUT_DIR / "_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total_files': len(md_files),
            'processed': len(results),
            'skipped': skipped,
            'total_cost': total_cost,
            'elapsed_seconds': elapsed.total_seconds(),
            'results': results
        }, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜: {stats_file}")
    print()
    print("âœ¨ å®Œæˆï¼æ£€æŸ¥ _enhanced/ ç›®å½•æŸ¥çœ‹å¢å¼ºåçš„æ–‡ç« ")

if __name__ == "__main__":
    main()
