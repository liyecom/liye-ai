#!/usr/bin/env python3
"""
æ”¹è¿›çš„å–å®¶ç²¾çµå†…å®¹æŠ“å–è„šæœ¬
ä½¿ç”¨æ›´æ™ºèƒ½çš„å†…å®¹è¯†åˆ«é€»è¾‘ï¼Œæ’é™¤å¯¼èˆªèœå•
"""

from playwright.sync_api import sync_playwright
import time
from pathlib import Path
import re

def clean_navigation_noise(text: str) -> str:
    """æ¸…ç†å¯¼èˆªèœå•å™ªéŸ³"""
    # å¸¸è§çš„å¯¼èˆªå…³é”®è¯
    nav_keywords = [
        'ä¸­æ–‡', 'æ—¥æœ¬èª', 'é¦–é¡µ', 'åå°', 'AIè§£è¯»', 'äº§å“', 'ä»·æ ¼',
        'ä¼˜éº¦äº‘', 'çŸ¥è¯†åº“', 'å¿«é€Ÿå…¥é—¨', 'è§†é¢‘è¯¾å ‚', 'åŠŸèƒ½æ‰‹å†Œ',
        'è¿è¥å¹²è´§', 'å®¢æœå’¨è¯¢', 'è¾¾äººæ‹›å‹Ÿ', 'åŠ å…¥æˆ‘ä»¬',
        'ç²¾çµçŸ¥è¯†åº“', 'ä»è¿™é‡Œå¼€å¯', 'æ´»åŠ¨ HOT', 'åª’ä½“æŠ¥é“',
        'è£èª‰å¥–é¡¹', 'å±•ä¼šé£é‡‡', 'å“ç‰Œ', 'ç¤¾åŒº', 'ç›´æ’­',
        'å¤§æ•°æ®é€‰å“', 'å…³é”®è¯ä¼˜åŒ–', 'è¿è¥æ¨å¹¿', 'æµè§ˆå™¨æ’ä»¶',
        'å…è´¹å·¥å…·', 'å‰å¾€åŠŸèƒ½æ‰‹å†Œ', 'è¡Œä¸šèµ„è®¯', 'æŸ¥çœ‹æ›´å¤š',
        'åº§æœº', 'å¾®ä¿¡å…¬ä¼—å·', 'æ‰«ç '
    ]

    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        line = line.strip()
        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue
        # è·³è¿‡å¯¼èˆªå…³é”®è¯è¡Œ
        if any(kw in line for kw in nav_keywords):
            continue
        # è·³è¿‡å¤ªçŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯å¯¼èˆªï¼‰
        if len(line) < 10:
            continue

        cleaned_lines.append(line)

    return '\n\n'.join(cleaned_lines)


def fetch_article_content(page, url):
    """è·å–å•ç¯‡æ–‡ç« å†…å®¹"""
    try:
        print(f"   æ­£åœ¨æŠ“å–...")
        page.goto(url, wait_until="domcontentloaded", timeout=60000)

        # ç­‰å¾…å†…å®¹åŠ è½½
        time.sleep(3)

        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        article_body = None

        # ç­–ç•¥1: æŸ¥æ‰¾ main æ ‡ç­¾å†…çš„å†…å®¹
        if page.locator('main').count() > 0:
            article_body = page.locator('main').first

        # ç­–ç•¥2: æŸ¥æ‰¾åŒ…å«å¤§æ®µæ–‡å­—çš„ div
        if not article_body or len(article_body.inner_text()) < 500:
            # æŸ¥æ‰¾æ‰€æœ‰ divï¼Œé€‰æ‹©æ–‡æœ¬æœ€é•¿çš„
            divs = page.locator('div').all()
            longest_div = None
            max_length = 0

            for div in divs:
                try:
                    text = div.inner_text()
                    if len(text) > max_length:
                        max_length = len(text)
                        longest_div = div
                except:
                    continue

            if longest_div and max_length > 1000:
                article_body = longest_div

        if not article_body:
            print(f"   âš ï¸  æ— æ³•æ‰¾åˆ°æ–‡ç« ä¸»ä½“")
            return None

        # æå–æ ‡é¢˜
        try:
            title = page.locator('h1').first.text_content().strip()
        except:
            title = "æœªçŸ¥æ ‡é¢˜"

        # æå–å†…å®¹å¹¶æ¸…ç†
        full_text = article_body.inner_text()
        cleaned_text = clean_navigation_noise(full_text)

        # æ£€æŸ¥æ¸…ç†åçš„å†…å®¹é•¿åº¦
        if len(cleaned_text) < 500:
            print(f"   âš ï¸  æ¸…ç†åå†…å®¹å¤ªçŸ­ï¼ˆ{len(cleaned_text)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½æ˜¯å¯¼èˆªé¡µ")
            return None

        # è½¬æ¢ä¸º Markdown
        markdown = f"# {title}\n\n{cleaned_text}\n"

        char_count = len(markdown)
        print(f"   âœ… æŠ“å–æˆåŠŸï¼ˆ{char_count} å­—ç¬¦ï¼Œæ¸…ç†åï¼‰")

        return {
            'title': title,
            'content': markdown,
            'url': url,
            'char_count': char_count
        }

    except Exception as e:
        print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
        return None


def fetch_article_list(page, start_index=0):
    """è·å–æ–‡ç« åˆ—è¡¨"""
    url = f"https://www.sellersprite.com/cn/blog?startIndex={start_index}"

    try:
        print(f"ğŸ“‹ æ­£åœ¨è®¿é—®: {url}")
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_selector('.article', timeout=15000)
        time.sleep(2)

        articles = page.locator('.article').all()
        article_list = []

        for article in articles:
            try:
                title = article.locator('.article-title').text_content().strip()
                link = article.locator('a').first.get_attribute('href')

                if link and link.startswith('/'):
                    link = f"https://www.sellersprite.com{link}"

                article_list.append({
                    'title': title,
                    'url': link
                })
            except:
                continue

        print(f"âœ… æ‰¾åˆ° {len(article_list)} ç¯‡æ–‡ç« ")
        return article_list

    except Exception as e:
        print(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        return []


def save_to_markdown(article, output_dir):
    """ä¿å­˜ä¸º Markdown æ–‡ä»¶"""
    # ç”Ÿæˆæ–‡ä»¶å
    filename = re.sub(r'[^\w\s-]', '', article['title'])
    filename = re.sub(r'[-\s]+', '-', filename)
    filename = f"{filename[:50]}.md"

    filepath = output_dir / filename

    # æ·»åŠ å…ƒä¿¡æ¯
    content = f"""---
source: å–å®¶ç²¾çµ
source_url: {article['url']}
fetched_at: {time.strftime('%Y-%m-%d')}
char_count: {article['char_count']}
---

{article['content']}

---

**æ¥æº**: [å–å®¶ç²¾çµ]({article['url']})
"""

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def main():
    print("ğŸš€ å¼€å§‹æŠ“å–å–å®¶ç²¾çµåšå®¢ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\\n")

    output_dir = Path.home() / 'github/liye_os/tools/web-publisher/fetched_articles_v2'
    output_dir.mkdir(parents=True, exist_ok=True)

    with sync_playwright() as p:
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        try:
            # å…ˆæµ‹è¯•å•é¡µï¼ˆ10 ç¯‡æ–‡ç« ï¼‰
            articles = fetch_article_list(page, start_index=0)

            if not articles:
                print("âŒ æœªæ‰¾åˆ°æ–‡ç« ")
                return

            # é™åˆ¶ä¸ºå‰ 10 ç¯‡ï¼ˆæµ‹è¯•ï¼‰
            articles = articles[:10]
            total = len(articles)

            success = 0
            failed = 0

            for i, article in enumerate(articles, 1):
                print(f"\\n[{i}/{total}] {article['title']}")

                content = fetch_article_content(page, article['url'])

                if content:
                    filepath = save_to_markdown(content, output_dir)
                    print(f"   ğŸ’¾ å·²ä¿å­˜: {filepath.name}")
                    success += 1
                else:
                    failed += 1

                # ç¤¼è²Œå»¶è¿Ÿ
                time.sleep(2)

            print(f"\\nâœ… æŠ“å–å®Œæˆ")
            print(f"   æˆåŠŸ: {success}")
            print(f"   å¤±è´¥: {failed}")
            print(f"   ä¿å­˜ä½ç½®: {output_dir}")

        finally:
            browser.close()
            print("\\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")


if __name__ == '__main__':
    main()
