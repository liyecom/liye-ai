#!/usr/bin/env python3
"""
ä» 296 ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡ºå‰ 50 ç¯‡æœ€æœ‰ä»·å€¼çš„
"""

import re
from pathlib import Path


def analyze_document_value(file_path: Path) -> dict:
    """åˆ†ææ–‡æ¡£ä»·å€¼"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åŸºç¡€æŒ‡æ ‡
        char_count = len(content)
        lines = [line for line in content.split('\n') if line.strip()]

        # æ ‡é¢˜æ•°é‡ï¼ˆç»“æ„æ€§æŒ‡æ ‡ï¼‰
        headings = re.findall(r'^#{1,6}\s+.+$', content, re.MULTILINE)

        # æ®µè½æ•°é‡
        paragraphs = [p for p in content.split('\n\n') if p.strip() and len(p.strip()) > 50]

        # å®Œæ•´å¥å­
        sentences = re.findall(r'[^ã€‚ï¼ï¼Ÿ\.\?!]+[ã€‚ï¼ï¼Ÿ\.\?!]', content)

        # è¡¨æ ¼æ•°é‡ï¼ˆè¡¨æ ¼å¤šçš„é€šå¸¸æ˜¯å·¥å…·/æ¨¡æ¿ï¼Œä¸æ˜¯æ•™ç¨‹ï¼‰
        tables = content.count('|---')

        # åˆ—è¡¨é¡¹
        list_items = re.findall(r'^\s*[-*]\s+', content, re.MULTILINE)

        # è´¨é‡è¯„åˆ†
        score = 0

        # 1. é•¿åº¦è¯„åˆ†ï¼ˆ20åˆ†ï¼‰- å€¾å‘ä¸­ç­‰é•¿åº¦çš„æ•™ç¨‹
        if 3000 <= char_count <= 20000:
            score += 20
        elif 1500 <= char_count < 3000:
            score += 15
        elif 20000 < char_count <= 50000:
            score += 10
        elif char_count > 50000:
            score += 5  # å¤ªé•¿å¯èƒ½æ˜¯è¡¨æ ¼

        # 2. ç»“æ„è¯„åˆ†ï¼ˆ25åˆ†ï¼‰- æœ‰æ¸…æ™°çš„ç« èŠ‚ç»“æ„
        if len(headings) >= 5:
            score += 25
        elif len(headings) >= 3:
            score += 15
        elif len(headings) >= 1:
            score += 5

        # 3. æ®µè½è¯„åˆ†ï¼ˆ20åˆ†ï¼‰- æœ‰å……å®çš„å†…å®¹æ®µè½
        if len(paragraphs) >= 10:
            score += 20
        elif len(paragraphs) >= 5:
            score += 15
        elif len(paragraphs) >= 2:
            score += 5

        # 4. å¥å­è¯„åˆ†ï¼ˆ15åˆ†ï¼‰- æœ‰å®Œæ•´çš„å™è¿°
        if len(sentences) >= 20:
            score += 15
        elif len(sentences) >= 10:
            score += 10
        elif len(sentences) >= 5:
            score += 5

        # 5. è¡¨æ ¼æƒ©ç½šï¼ˆ-20åˆ†ï¼‰- è¡¨æ ¼å¤ªå¤šè¯´æ˜æ˜¯å·¥å…·è€Œéæ•™ç¨‹
        if tables > 20:
            score -= 20
        elif tables > 10:
            score -= 10
        elif tables > 5:
            score -= 5

        # 6. åˆ—è¡¨åŠ åˆ†ï¼ˆ10åˆ†ï¼‰- æœ‰æ­¥éª¤æˆ–è¦ç‚¹
        if len(list_items) >= 10:
            score += 10
        elif len(list_items) >= 5:
            score += 5

        # 7. æ–‡ä»¶ååŠ åˆ†ï¼ˆ10åˆ†ï¼‰- ä¼˜å…ˆå®Œæ•´ç‰ˆã€æ”»ç•¥ã€æŒ‡å—
        if 'å®Œæ•´ç‰ˆ' in file_path.name:
            score += 10
        elif any(word in file_path.name for word in ['æ”»ç•¥', 'æŒ‡å—', 'è¯¦è§£', 'å®æˆ˜', 'æŠ€å·§']):
            score += 5

        # å†…å®¹ç±»å‹åˆ¤æ–­
        if tables > 10 and len(paragraphs) < 5:
            content_type = "å·¥å…·/è¡¨æ ¼"
        elif 'å®Œæ•´ç‰ˆ' in file_path.name or len(paragraphs) >= 5:
            content_type = "æ•™ç¨‹/æŒ‡å—"
        elif len(list_items) >= 10:
            content_type = "æ¸…å•/è¦ç‚¹"
        else:
            content_type = "å…¶ä»–"

        return {
            'path': file_path,
            'name': file_path.name,
            'score': score,
            'char_count': char_count,
            'headings': len(headings),
            'paragraphs': len(paragraphs),
            'sentences': len(sentences),
            'tables': tables,
            'list_items': len(list_items),
            'content_type': content_type,
        }

    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {file_path.name} - {e}")
        return None


def main():
    posts_dir = Path.home() / 'github/liye_os/websites/amazon-optimization/src/content/posts'

    print(f"ğŸ” åˆ†ææ‰€æœ‰æ–‡æ¡£å¹¶ç­›é€‰å‰ 50 ç¯‡...\\n   ç›®å½•: {posts_dir}\\n")

    md_files = list(posts_dir.glob('*.md'))
    results = []

    for file_path in md_files:
        result = analyze_document_value(file_path)
        if result:
            results.append(result)

    # æŒ‰è¯„åˆ†æ’åº
    results.sort(key=lambda x: x['score'], reverse=True)

    print(f"\\nğŸ“Š åˆ†æå®Œæˆï¼šå…± {len(results)} ä¸ªæ–‡æ¡£")

    # ç»Ÿè®¡å†…å®¹ç±»å‹
    content_types = {}
    for r in results:
        ct = r['content_type']
        content_types[ct] = content_types.get(ct, 0) + 1

    print(f"\\nğŸ“‹ å†…å®¹ç±»å‹åˆ†å¸ƒï¼š")
    for ct, count in sorted(content_types.items(), key=lambda x: x[1], reverse=True):
        print(f"   {ct}: {count} ç¯‡")

    # å‰ 50 ç¯‡
    top_50 = results[:50]

    print(f"\\nâ­ å‰ 50 ç¯‡é«˜è´¨é‡æ–‡æ¡£ï¼š")
    for i, r in enumerate(top_50, 1):
        print(f"   {i}. {r['name']}")
        print(f"      è¯„åˆ†: {r['score']} | ç±»å‹: {r['content_type']} | å­—æ•°: {r['char_count']} | æ®µè½: {r['paragraphs']}")

    # åˆ é™¤å…¶ä»–æ–‡æ¡£
    print(f"\\nğŸ—‘ï¸  åˆ é™¤å…¶ä»– {len(results) - 50} ä¸ªæ–‡æ¡£...")
    deleted = 0
    for r in results[50:]:
        r['path'].unlink()
        deleted += 1

    print(f"\\nâœ… æ¸…ç†å®Œæˆ")
    print(f"   ä¿ç•™: 50 ç¯‡é«˜è´¨é‡æ–‡æ¡£")
    print(f"   åˆ é™¤: {deleted} ç¯‡ä½è´¨é‡æ–‡æ¡£")

    # ä¿å­˜ top 50 åˆ—è¡¨
    output_file = Path.home() / 'github/liye_os/tools/web-publisher/top_50_docs.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        for r in top_50:
            f.write(f"{r['name']}\\n")

    print(f"\\nğŸ’¾ å‰ 50 ç¯‡æ–‡æ¡£åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")


if __name__ == '__main__':
    main()
