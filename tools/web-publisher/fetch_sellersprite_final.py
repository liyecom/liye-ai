#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå–å®¶ç²¾çµå†…å®¹æŠ“å–è„šæœ¬
ç²¾å‡†æå–æ–‡ç« æ­£æ–‡ï¼Œæ’é™¤æ‰€æœ‰å™ªéŸ³
"""

from playwright.sync_api import sync_playwright
import time
from pathlib import Path
import re

def extract_article_body(full_text: str, title: str) -> str:
    """
    ç²¾å‡†æå–æ–‡ç« æ­£æ–‡
    ç­–ç•¥ï¼šæ‰¾åˆ°æ ‡é¢˜åçš„å†…å®¹ï¼Œç›´åˆ°é‡åˆ°è¯„è®º/é¡µè„šç­‰ç»“æŸæ ‡è®°
    """
    lines = full_text.split('\n')

    # ç»“æŸæ ‡è®°
    end_markers = [
        'ç‚¹èµè¯¦æƒ…', 'å…¨éƒ¨è¯„è®º', 'æœ€æ–° (', 'æœ€çƒ­ (',
        'å·¥ä½œæ—¶é—´ï¼š', 'é‚®ç®±ï¼š', 'å®¢æœï¼š', 'å¸‚åœºåˆä½œï¼š',
        'ç‰ˆæƒæ‰€æœ‰', 'èœ€ICPå¤‡', 'å·å…¬ç½‘å®‰å¤‡',
        'æˆéƒ½äº‘é›…ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸',
        'ä¸»äºº~æ‚¨è¿˜æ²¡æœ‰æ”¶è—çš„å·¥å…·'
    ]

    # å¯¼èˆªå™ªéŸ³å…³é”®è¯ï¼ˆéœ€è¦è·³è¿‡çš„è¡Œï¼‰
    nav_keywords = [
        'ä¸­æ–‡', 'æ—¥æœ¬èª', 'é¦–é¡µ', 'åå°', 'AIè§£è¯»', 'äº§å“', 'ä»·æ ¼',
        'ä¼˜éº¦äº‘', 'çŸ¥è¯†åº“', 'å¿«é€Ÿå…¥é—¨', 'è§†é¢‘è¯¾å ‚', 'åŠŸèƒ½æ‰‹å†Œ',
        'è¿è¥å¹²è´§', 'å®¢æœå’¨è¯¢', 'è¾¾äººæ‹›å‹Ÿ', 'åŠ å…¥æˆ‘ä»¬',
        'ç²¾çµçŸ¥è¯†åº“', 'ä»è¿™é‡Œå¼€å¯', 'æ´»åŠ¨ HOT', 'åª’ä½“æŠ¥é“',
        'è£èª‰å¥–é¡¹', 'å±•ä¼šé£é‡‡', 'å“ç‰Œ', 'ç¤¾åŒº', 'ç›´æ’­',
        'å¤§æ•°æ®é€‰å“', 'å…³é”®è¯ä¼˜åŒ–', 'è¿è¥æ¨å¹¿', 'æµè§ˆå™¨æ’ä»¶',
        'å…è´¹å·¥å…·', 'å‰å¾€åŠŸèƒ½æ‰‹å†Œ', 'è¡Œä¸šèµ„è®¯', 'æŸ¥çœ‹æ›´å¤š',
        'åº§æœº', 'å¾®ä¿¡å…¬ä¼—å·', 'æ‰«ç ', 'è§†é¢‘ç‰ˆ', 'å„åŠŸèƒ½è¯¦è§£',
        'å›¾ç‰‡æ¥æºï¼šå–å®¶ç²¾çµ', 'æ’ä»¶ä¸‹è½½', 'å¥—é¤è´­ä¹°', 'å¸¸è§é—®é¢˜',
        'å­è´¦å·', 'æ•°æ®æ›´æ–°', 'é˜…è¯»æ•°(', 'è¯„è®ºæ•°(',
        'å¾®ä¿¡æ‰«ä¸€æ‰«', 'è®©æ¯ä¸€æ¬¡åˆä½œ', 'å…³é”®è¯è½¬åŒ–ç‡',
        'Listingç”Ÿæˆå™¨', 'Google Trends', 'Keepaæ’ä»¶',
        'èµ¶å¿«ä»å³ä¾§å·¥å…·æ·»åŠ å§', 'ç”¨äºå¿«é€Ÿè®¿é—®å–œçˆ±çš„å·¥å…·',
        '028-', '139-', '400-', '186-', '189-'  # ç”µè¯å·ç 
    ]

    # æ‰¾åˆ°æ­£æ–‡å¼€å§‹ä½ç½®ï¼ˆæ ‡é¢˜ä¹‹åçš„ç¬¬ä¸€æ®µæœ‰æ•ˆå†…å®¹ï¼‰
    start_idx = -1
    for i, line in enumerate(lines):
        line = line.strip()

        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue

        # è·³è¿‡å¯¼èˆªå™ªéŸ³
        if any(kw in line for kw in nav_keywords):
            continue

        # è·³è¿‡å¤ªçŸ­çš„è¡Œ
        if len(line) < 15:
            continue

        # è·³è¿‡æ—¥æœŸè¡Œ
        if re.match(r'^\d{4}/\d{1,2}/\d{1,2}', line):
            continue

        # æ‰¾åˆ°ç¬¬ä¸€æ®µæœ‰æ•ˆå†…å®¹ï¼ˆé€šå¸¸ä»¥"å¯¹äº"ã€"åœ¨"ã€"éšç€"ç­‰å¼€å¤´ï¼‰
        if len(line) > 30 and any(line.startswith(prefix) for prefix in ['å¯¹äº', 'åœ¨', 'éšç€', 'è¿‘å¹´æ¥', 'æœ¬æ–‡', 'äºšé©¬é€Š', 'ä½œä¸º']):
            start_idx = i
            break

    if start_idx == -1:
        return ""

    # ä»å¼€å§‹ä½ç½®æå–å†…å®¹ï¼Œç›´åˆ°é‡åˆ°ç»“æŸæ ‡è®°
    article_lines = []
    for i in range(start_idx, len(lines)):
        line = lines[i].strip()

        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç»“æŸæ ‡è®°
        if any(marker in line for marker in end_markers):
            break

        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue

        # è·³è¿‡å¯¼èˆªå™ªéŸ³
        if any(kw in line for kw in nav_keywords):
            continue

        # è·³è¿‡å›¾ç‰‡è¯´æ˜
        if line.startswith('ï¼ˆå›¾ç‰‡æ¥æº'):
            continue

        article_lines.append(line)

    return '\n\n'.join(article_lines)


def fetch_article_content(page, url):
    """è·å–å•ç¯‡æ–‡ç« å†…å®¹"""
    try:
        print(f"   æ­£åœ¨æŠ“å–...")
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        time.sleep(3)

        # æå–æ ‡é¢˜
        try:
            title = page.locator('h1').first.text_content().strip()
        except:
            title = "æœªçŸ¥æ ‡é¢˜"

        # è·å–æ•´ä¸ªé¡µé¢æ–‡æœ¬
        full_text = page.locator('body').inner_text()

        # ç²¾å‡†æå–æ–‡ç« æ­£æ–‡
        article_body = extract_article_body(full_text, title)

        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(article_body) < 300:
            print(f"   âš ï¸  æå–å†…å®¹å¤ªçŸ­ï¼ˆ{len(article_body)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½ä¸æ˜¯æ–‡ç« é¡µ")
            return None

        # è½¬æ¢ä¸º Markdown
        markdown = f"# {title}\n\n{article_body}\n"

        char_count = len(markdown)
        print(f"   âœ… æŠ“å–æˆåŠŸï¼ˆ{char_count} å­—ç¬¦ï¼‰")

        return {
            'title': title,
            'content': markdown,
            'url': url,
            'char_count': char_count
        }

    except Exception as e:
        print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
        return None


def fetch_article_list(page, start_index=0):
    """è·å–æ–‡ç« åˆ—è¡¨"""
    url = f"https://www.sellersprite.com/cn/blog?startIndex={start_index}"

    try:
        print(f"ğŸ“‹ æ­£åœ¨è®¿é—®: {url}")
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        page.wait_for_selector('.article', timeout=15000)
        time.sleep(2)

        articles = page.locator('.article').all()
        article_list = []

        for article in articles:
            try:
                title = article.locator('.article-title').text_content().strip()
                link = article.locator('a').first.get_attribute('href')

                if link and link.startswith('/'):
                    link = f"https://www.sellersprite.com{link}"

                article_list.append({
                    'title': title,
                    'url': link
                })
            except:
                continue

        print(f"âœ… æ‰¾åˆ° {len(article_list)} ç¯‡æ–‡ç« ")
        return article_list

    except Exception as e:
        print(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {e}")
        return []


def save_to_markdown(article, output_dir):
    """ä¿å­˜ä¸º Markdown æ–‡ä»¶"""
    # ç”Ÿæˆæ–‡ä»¶å
    filename = re.sub(r'[^\w\s-]', '', article['title'])
    filename = re.sub(r'[-\s]+', '-', filename)
    filename = f"{filename[:50]}.md"

    filepath = output_dir / filename

    # æ·»åŠ å…ƒä¿¡æ¯
    content = f"""---
source: å–å®¶ç²¾çµ
source_url: {article['url']}
fetched_at: {time.strftime('%Y-%m-%d')}
char_count: {article['char_count']}
---

{article['content']}

---

**æ¥æº**: [å–å®¶ç²¾çµ]({article['url']})
"""

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

    return filepath


def main():
    print("ğŸš€ å¼€å§‹æŠ“å–å–å®¶ç²¾çµåšå®¢ï¼ˆæœ€ç»ˆç‰ˆï¼‰...\n")

    output_dir = Path.home() / 'github/liye_os/tools/web-publisher/fetched_articles_final'
    output_dir.mkdir(parents=True, exist_ok=True)

    with sync_playwright() as p:
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        try:
            # æŠ“å– 5 é¡µï¼ˆçº¦ 50 ç¯‡æ–‡ç« ï¼‰
            all_articles = []
            for page_num in range(5):
                start_index = page_num * 11
                articles = fetch_article_list(page, start_index=start_index)

                if not articles:
                    break

                all_articles.extend(articles)
                print(f"å·²è·å– {len(all_articles)} ç¯‡æ–‡ç« ...\n")

            if not all_articles:
                print("âŒ æœªæ‰¾åˆ°æ–‡ç« ")
                return

            # é™åˆ¶ä¸ºå‰ 50 ç¯‡
            all_articles = all_articles[:50]
            total = len(all_articles)

            success = 0
            failed = 0

            for i, article in enumerate(all_articles, 1):
                print(f"[{i}/{total}] {article['title']}")

                content = fetch_article_content(page, article['url'])

                if content:
                    filepath = save_to_markdown(content, output_dir)
                    print(f"   ğŸ’¾ å·²ä¿å­˜: {filepath.name}\n")
                    success += 1
                else:
                    failed += 1

                # ç¤¼è²Œå»¶è¿Ÿ
                time.sleep(2)

            print(f"âœ… æŠ“å–å®Œæˆ")
            print(f"   æˆåŠŸ: {success}")
            print(f"   å¤±è´¥: {failed}")
            print(f"   ä¿å­˜ä½ç½®: {output_dir}")

        finally:
            browser.close()
            print("\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")


if __name__ == '__main__':
    main()
