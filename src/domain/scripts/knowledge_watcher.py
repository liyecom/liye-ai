#!/usr/bin/env python3
"""
çŸ¥è¯†åº“æ–‡ä»¶ç›‘æ§æœåŠ¡
è‡ªåŠ¨æ£€æµ‹ MD æ–‡ä»¶å˜åŒ–å¹¶å¢é‡ç´¢å¼•åˆ° Qdrant
"""

import sys
import time
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_dir = Path(__file__).parent.parent
sys.path.insert(0, str(project_dir))

from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from tools.simple_embedder import SimpleEmbedder
from scripts.build_global_index import MarkdownChunker

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/knowledge_watcher.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class KnowledgeIndexHandler(FileSystemEventHandler):
    """å¤„ç†çŸ¥è¯†åº“æ–‡ä»¶å˜åŒ–"""

    def __init__(self):
        super().__init__()
        self.qdrant = QdrantClient(url="http://localhost:6333")
        self.embedder = SimpleEmbedder()
        self.chunker = MarkdownChunker(chunk_size=800, overlap=100)

        # å®šä¹‰çŸ¥è¯†åº“æ˜ å°„ï¼ˆç›®å½• â†’ collectionï¼‰
        self.source_mappings = {
            Path.home() / "Documents/å‡ºæµ·è·¨å¢ƒ/Amazon": "amazon_knowledge_base",
            Path.home() / "Documents/ç”Ÿè´¢æœ‰æœ¯": "shengcai_library",
            Path.home() / "Documents/ç™Œç—‡é¢†åŸŸ": "medical_resources",
            Path.home() / "Documents/liye_workspace/LiYe_OS/Skills": "liye_os_skills",
            Path.home() / "Documents/å‡ºæµ·è·¨å¢ƒ": "crossborder_ecommerce",
            Path.home() / "Documents/liye_workspace/20 Areas": "para_areas"
        }

        # é˜²æŠ–ï¼šé¿å…çŸ­æ—¶é—´å†…å¤šæ¬¡è§¦å‘
        self.last_indexed = {}
        self.debounce_seconds = 5

    def _get_collection_for_file(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„ç¡®å®šåº”è¯¥ç´¢å¼•åˆ°å“ªä¸ª collection"""
        for source_dir, collection_name in self.source_mappings.items():
            try:
                file_path.relative_to(source_dir)
                return collection_name
            except ValueError:
                continue
        return None

    def _should_index(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«ç´¢å¼•"""
        # åªç´¢å¼• MD æ–‡ä»¶
        if file_path.suffix.lower() != '.md':
            return False

        # æ’é™¤éšè—æ–‡ä»¶å’Œç›®å½•
        if any(part.startswith('.') for part in file_path.parts):
            return False

        # é˜²æŠ–æ£€æŸ¥
        file_key = str(file_path)
        last_time = self.last_indexed.get(file_key, 0)
        current_time = time.time()

        if current_time - last_time < self.debounce_seconds:
            return False

        self.last_indexed[file_key] = current_time
        return True

    def _index_file(self, file_path: Path):
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            # ç¡®å®šç›®æ ‡ collection
            collection_name = self._get_collection_for_file(file_path)
            if not collection_name:
                logger.warning(f"è·³è¿‡æ–‡ä»¶ï¼ˆä¸åœ¨ç›‘æ§ç›®å½•ï¼‰: {file_path}")
                return

            # è¯»å–æ–‡ä»¶å†…å®¹
            content = file_path.read_text(encoding='utf-8')

            # åˆ†å—
            chunks = self.chunker.chunk_by_headers(content, str(file_path))

            # ç”Ÿæˆå‘é‡å¹¶ä¸Šä¼ 
            points = []
            for chunk in chunks:
                # ç”Ÿæˆå”¯ä¸€ ID
                file_chunk_id = f"{file_path.stem}_{chunk['chunk_id']}"
                point_id = hashlib.md5(file_chunk_id.encode()).hexdigest()

                # ç”Ÿæˆ embedding
                text_content = str(chunk.get('text', ''))
                if not text_content.strip():
                    continue

                embedding = self.embedder.embed_text(text_content)

                # åˆ›å»º point
                text_preview = text_content[:500] if text_content else ''
                point = PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        'source': collection_name,
                        'file_path': file_path.name,
                        'full_path': str(file_path),
                        'chunk_id': str(chunk.get('chunk_id', '')),
                        'chunk_type': chunk.get('type', 'unknown'),
                        'text': text_preview,
                        'text_length': len(text_content),
                        'indexed_at': datetime.now().isoformat()
                    }
                )
                points.append(point)

            # æ‰¹é‡ä¸Šä¼ 
            if points:
                self.qdrant.upsert(
                    collection_name=collection_name,
                    points=points
                )
                logger.info(f"âœ“ å·²ç´¢å¼•: {file_path.name} â†’ {collection_name} ({len(points)} chunks)")
            else:
                logger.warning(f"âš  æ–‡ä»¶æ— æœ‰æ•ˆå†…å®¹: {file_path.name}")

        except Exception as e:
            logger.error(f"âœ— ç´¢å¼•å¤±è´¥: {file_path.name} - {e}")

    def _delete_file_from_index(self, file_path: Path):
        """ä»ç´¢å¼•ä¸­åˆ é™¤æ–‡ä»¶"""
        try:
            collection_name = self._get_collection_for_file(file_path)
            if not collection_name:
                return

            # TODO: å®ç°åˆ é™¤é€»è¾‘ï¼ˆéœ€è¦æ ¹æ® full_path æŸ¥è¯¢å¹¶åˆ é™¤æ‰€æœ‰ç›¸å…³ pointsï¼‰
            # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸º Qdrant åˆ é™¤éœ€è¦å…ˆæŸ¥è¯¢ ID
            logger.info(f"â„¹ æ–‡ä»¶å·²åˆ é™¤ï¼ˆç´¢å¼•æš‚æœªæ¸…ç†ï¼‰: {file_path.name}")

        except Exception as e:
            logger.error(f"âœ— åˆ é™¤ç´¢å¼•å¤±è´¥: {file_path.name} - {e}")

    def on_created(self, event):
        """å¤„ç†æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if isinstance(event, FileCreatedEvent) and not event.is_directory:
            file_path = Path(event.src_path)
            if self._should_index(file_path):
                logger.info(f"ğŸ†• æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {file_path.name}")
                self._index_file(file_path)

    def on_modified(self, event):
        """å¤„ç†æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        if isinstance(event, FileModifiedEvent) and not event.is_directory:
            file_path = Path(event.src_path)
            if self._should_index(file_path):
                logger.info(f"âœï¸  æ£€æµ‹åˆ°æ–‡ä»¶ä¿®æ”¹: {file_path.name}")
                self._index_file(file_path)

    def on_deleted(self, event):
        """å¤„ç†æ–‡ä»¶åˆ é™¤äº‹ä»¶"""
        if isinstance(event, FileDeletedEvent) and not event.is_directory:
            file_path = Path(event.src_path)
            if file_path.suffix.lower() == '.md':
                logger.info(f"ğŸ—‘ï¸  æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤: {file_path.name}")
                self._delete_file_from_index(file_path)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ çŸ¥è¯†åº“ç›‘æ§æœåŠ¡å¯åŠ¨")
    logger.info("=" * 60)

    # æ£€æŸ¥ Qdrant è¿æ¥
    try:
        qdrant = QdrantClient(url="http://localhost:6333")
        collections = qdrant.get_collections()
        logger.info(f"âœ“ Qdrant è¿æ¥æˆåŠŸï¼Œå½“å‰æœ‰ {len(collections.collections)} ä¸ª collections")
    except Exception as e:
        logger.error(f"âœ— æ— æ³•è¿æ¥åˆ° Qdrant: {e}")
        logger.error("è¯·ç¡®ä¿ Docker å®¹å™¨æ­£åœ¨è¿è¡Œ: docker ps | grep qdrant")
        sys.exit(1)

    # åˆ›å»ºç›‘æ§å™¨
    event_handler = KnowledgeIndexHandler()
    observer = Observer()

    # ç›‘æ§æ‰€æœ‰çŸ¥è¯†åº“ç›®å½•
    watch_dirs = list(event_handler.source_mappings.keys())
    logger.info(f"ğŸ“‚ ç›‘æ§ {len(watch_dirs)} ä¸ªç›®å½•:")

    for watch_dir in watch_dirs:
        if watch_dir.exists():
            observer.schedule(event_handler, str(watch_dir), recursive=True)
            logger.info(f"   âœ“ {watch_dir}")
        else:
            logger.warning(f"   âš  ç›®å½•ä¸å­˜åœ¨: {watch_dir}")

    # å¯åŠ¨ç›‘æ§
    observer.start()
    logger.info("=" * 60)
    logger.info("ğŸ‘€ å¼€å§‹ç›‘æ§æ–‡ä»¶å˜åŒ–... (æŒ‰ Ctrl+C åœæ­¢)")
    logger.info("=" * 60)

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
        observer.stop()

    observer.join()
    logger.info("âœ“ ç›‘æ§æœåŠ¡å·²åœæ­¢")


if __name__ == "__main__":
    main()
