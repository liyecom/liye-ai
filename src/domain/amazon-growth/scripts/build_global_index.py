#!/usr/bin/env python3
"""
å…¨å±€çŸ¥è¯†åº“ç´¢å¼•è„šæœ¬
ä¸º Obsidian ç»Ÿä¸€çŸ¥è¯†ç®¡ç†åˆ›å»ºå‘é‡ç´¢å¼•
"""

import sys
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_dir = Path(__file__).parent.parent
sys.path.insert(0, str(project_dir))

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from tools.simple_embedder import SimpleEmbedder


class MarkdownChunker:
    """æ™ºèƒ½ Markdown åˆ†å—å™¨"""

    def __init__(self, chunk_size: int = 800, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_by_headers(self, content: str, file_path: str) -> List[Dict]:
        """æŒ‰ Markdown æ ‡é¢˜åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´"""
        chunks = []

        # æŒ‰äºŒçº§æ ‡é¢˜åˆ†å‰²
        sections = content.split('\n## ')

        for i, section in enumerate(sections):
            if not section.strip():
                continue

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œå¯èƒ½åŒ…å«ä¸€çº§æ ‡é¢˜
            if i == 0:
                text = section
            else:
                text = '## ' + section

            # å¦‚æœå•ä¸ªç« èŠ‚å¤ªé•¿ï¼ŒæŒ‰æ®µè½è¿›ä¸€æ­¥åˆ†å‰²
            if len(text) > self.chunk_size:
                sub_chunks = self._chunk_by_paragraphs(text)
                for j, sub_chunk in enumerate(sub_chunks):
                    chunks.append({
                        'text': sub_chunk,
                        'chunk_id': f"{i}_{j}",
                        'type': 'paragraph'
                    })
            else:
                chunks.append({
                    'text': text,
                    'chunk_id': str(i),
                    'type': 'section'
                })

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼ŒæŒ‰æ®µè½åˆ†å‰²
        if len(chunks) == 0:
            chunks = self._chunk_by_paragraphs(content)

        return chunks

    def _chunk_by_paragraphs(self, text: str) -> List[Dict]:
        """æŒ‰æ®µè½åˆ†å—"""
        chunks = []
        paragraphs = text.split('\n\n')

        current_chunk = ""
        chunk_id = 0

        for para in paragraphs:
            if not para.strip():
                continue

            # å¦‚æœæ·»åŠ è¿™ä¸ªæ®µè½ä¼šè¶…è¿‡é™åˆ¶
            if len(current_chunk) + len(para) > self.chunk_size:
                if current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'chunk_id': str(chunk_id),
                        'type': 'paragraph'
                    })
                    chunk_id += 1

                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†å‰²
                if len(para) > self.chunk_size:
                    for i in range(0, len(para), self.chunk_size - self.overlap):
                        chunk_text = para[i:i + self.chunk_size]
                        chunks.append({
                            'text': chunk_text,
                            'chunk_id': f"{chunk_id}_{i}",
                            'type': 'fragment'
                        })
                    current_chunk = ""
                else:
                    current_chunk = para + "\n\n"
            else:
                current_chunk += para + "\n\n"

        # æ·»åŠ æœ€åä¸€ä¸ª chunk
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'chunk_id': str(chunk_id),
                'type': 'paragraph'
            })

        return chunks


class GlobalIndexBuilder:
    """å…¨å±€çŸ¥è¯†åº“ç´¢å¼•æ„å»ºå™¨"""

    # å®šä¹‰è¦ç´¢å¼•çš„æº
    SOURCES = [
        {
            'name': 'liye_os_skills',
            'path': Path.home() / 'Documents/liye_workspace/LiYe_OS',
            'description': 'LiYe OS èƒ½åŠ›æ¡†æ¶å’Œ Skills'
        },
        {
            'name': 'para_areas',
            'path': Path.home() / 'Documents/liye_workspace/20 Areas',
            'description': 'PARA é•¿æœŸå…³æ³¨é¢†åŸŸç´¢å¼•'
        },
        {
            'name': 'crossborder_ecommerce',
            'path': Path.home() / 'Documents/å‡ºæµ·è·¨å¢ƒ',
            'description': 'è·¨å¢ƒç”µå•†å·¥ä½œåŒºï¼ˆAmazon/TikTok/ç‹¬ç«‹ç«™ï¼‰'
        },
        {
            'name': 'shengcai_library',
            'path': Path.home() / 'Documents/ç”Ÿè´¢æœ‰æœ¯',
            'description': 'ç”Ÿè´¢æœ‰æœ¯åˆ›ä¸šè´¢å•†çŸ¥è¯†åº“'
        },
        {
            'name': 'medical_resources',
            'path': Path.home() / 'Documents/ç™Œç—‡é¢†åŸŸ',
            'description': 'åŒ»ç–—å¥åº·èµ„æº'
        }
    ]

    def __init__(self, qdrant_url: str = "http://localhost:6333"):
        self.qdrant = QdrantClient(url=qdrant_url)
        self.embedder = SimpleEmbedder(model_name="all-MiniLM-L6-v2")
        self.chunker = MarkdownChunker(chunk_size=800, overlap=100)
        self.embedding_dim = self.embedder.get_embedding_dim()

        print(f"âœ“ Connected to Qdrant at {qdrant_url}")
        print(f"âœ“ Embedding dimension: {self.embedding_dim}")

    def create_collection(self, collection_name: str):
        """åˆ›å»ºæˆ–é‡å»º collection"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            collections = self.qdrant.get_collections().collections
            exists = any(c.name == collection_name for c in collections)

            if exists:
                print(f"âš ï¸  Collection '{collection_name}' already exists, recreating...")
                self.qdrant.delete_collection(collection_name)

            # åˆ›å»ºæ–° collection
            self.qdrant.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_dim,
                    distance=Distance.COSINE
                )
            )
            print(f"âœ“ Created collection: {collection_name}")

        except Exception as e:
            print(f"âœ— Error creating collection {collection_name}: {e}")
            raise

    def scan_markdown_files(self, source_path: Path) -> List[Path]:
        """æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰ Markdown æ–‡ä»¶"""
        if not source_path.exists():
            print(f"âš ï¸  Path does not exist: {source_path}")
            return []

        md_files = list(source_path.rglob("*.md"))
        print(f"  Found {len(md_files)} MD files in {source_path.name}")
        return md_files

    def index_file(self, file_path: Path, source_name: str) -> List[PointStruct]:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶ï¼Œè¿”å› points"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')

            # æ™ºèƒ½åˆ†å—
            chunks = self.chunker.chunk_by_headers(content, str(file_path))

            if not chunks:
                return []

            # ç”Ÿæˆ embeddingsï¼ˆæ‰¹é‡ï¼‰
            texts = [chunk['text'] for chunk in chunks]
            embeddings = self.embedder.embed_batch(texts, show_progress=False)

            # åˆ›å»º points
            points = []
            for chunk, embedding in zip(chunks, embeddings):
                # ç”Ÿæˆå”¯ä¸€ ID
                chunk_identifier = f"{file_path}::{chunk['chunk_id']}"
                point_id = hashlib.md5(chunk_identifier.encode()).hexdigest()

                # å®‰å…¨åœ°è·å–æ–‡æœ¬é¢„è§ˆ
                text_content = str(chunk.get('text', ''))
                text_preview = text_content[:500] if text_content else ''

                point = PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload={
                        'source': source_name,
                        'file_path': str(file_path.relative_to(file_path.parent.parent.parent)),
                        'full_path': str(file_path),
                        'chunk_id': str(chunk.get('chunk_id', '')),
                        'chunk_type': chunk.get('type', 'unknown'),
                        'text': text_preview,  # å­˜å‚¨å‰ 500 å­—ç¬¦é¢„è§ˆ
                        'text_length': len(text_content),
                        'indexed_at': datetime.now().isoformat()
                    }
                )
                points.append(point)

            return points

        except Exception as e:
            print(f"  âœ— Error indexing {file_path.name}: {e}")
            return []

    def index_source(self, source: Dict, batch_size: int = 50):
        """ç´¢å¼•ä¸€ä¸ªçŸ¥è¯†æº"""
        collection_name = source['name']
        source_path = source['path']

        print(f"\n{'='*60}")
        print(f"ç´¢å¼•çŸ¥è¯†æº: {source['description']}")
        print(f"Collection: {collection_name}")
        print(f"Path: {source_path}")
        print(f"{'='*60}\n")

        # åˆ›å»º collection
        self.create_collection(collection_name)

        # æ‰«ææ–‡ä»¶
        md_files = self.scan_markdown_files(source_path)

        if not md_files:
            print(f"âš ï¸  No files to index for {collection_name}")
            return

        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        all_points = []
        total_files = len(md_files)

        for i, md_file in enumerate(md_files, 1):
            if i % 10 == 0 or i == total_files:
                print(f"  Processing: {i}/{total_files} files...", end='\r')

            points = self.index_file(md_file, source['name'])
            all_points.extend(points)

            # æ¯ batch_size ä¸ªæ–‡ä»¶ä¸Šä¼ ä¸€æ¬¡
            if len(all_points) >= batch_size * 10:  # å‡è®¾æ¯ä¸ªæ–‡ä»¶å¹³å‡ 10 ä¸ª chunks
                self.qdrant.upsert(
                    collection_name=collection_name,
                    points=all_points
                )
                all_points = []

        # ä¸Šä¼ å‰©ä½™çš„ points
        if all_points:
            self.qdrant.upsert(
                collection_name=collection_name,
                points=all_points
            )

        # è·å–æœ€ç»ˆç»Ÿè®¡
        collection_info = self.qdrant.get_collection(collection_name)
        total_chunks = collection_info.points_count

        print(f"\nâœ… ç´¢å¼•å®Œæˆ:")
        print(f"   æ–‡ä»¶æ•°: {total_files}")
        print(f"   Chunks: {total_chunks}")
        print(f"   å¹³å‡æ¯æ–‡ä»¶: {total_chunks/total_files:.1f} chunks")

    def build_all(self):
        """æ„å»ºæ‰€æœ‰çŸ¥è¯†æºçš„ç´¢å¼•"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ„å»ºå…¨å±€çŸ¥è¯†åº“ç´¢å¼•")
        print("="*60 + "\n")

        start_time = datetime.now()

        for source in self.SOURCES:
            try:
                self.index_source(source)
            except Exception as e:
                print(f"\nâœ— Failed to index {source['name']}: {e}")
                print("Continuing with next source...\n")

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # æœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("âœ… å…¨å±€ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print("="*60 + "\n")

        collections = self.qdrant.get_collections().collections
        print("Collections:")
        for coll in collections:
            info = self.qdrant.get_collection(coll.name)
            print(f"  - {coll.name}: {info.points_count} chunks")

        print(f"\næ€»è€—æ—¶: {duration:.1f} ç§’ ({duration/60:.1f} åˆ†é’Ÿ)")


def main():
    """ä¸»å‡½æ•°"""
    try:
        builder = GlobalIndexBuilder()
        builder.build_all()

        print("\nâœ… æ‰€æœ‰ç´¢å¼•å·²åˆ›å»ºï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. åœ¨ Obsidian ä¸­æµ‹è¯•å…¨å±€æœç´¢")
        print("  2. é…ç½® File Watcher è‡ªåŠ¨å¢é‡ç´¢å¼•")
        print("  3. åˆ›å»º Obsidian è¯­ä¹‰æœç´¢æ’ä»¶é›†æˆ")

    except Exception as e:
        print(f"\nâœ— Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
