# LiYe AI Domain Configuration - GEO OS (Knowledge Engine)
# Location: src/domain/geo-os/config.yaml

domain:
  id: geo-os
  name: GEO OS Knowledge Engine
  version: 0.1.0
  description: Core knowledge extraction and processing engine for LiYe OS
  layer: core  # Note: This is core infrastructure, not application domain

  # Tri-Fork Integration
  method_layer: src/method/
  skill_layer: src/skill/
  runtime_layer: src/runtime/

# === System Position ===
architecture:
  position: core-infrastructure
  consumers:
    # Domain-specific consumers are in private repositories
    - skeleton           # Example consumer
    - medical-research   # Consumes geo_units.json
  provides:
    - geo_units.json     # Structured knowledge units
  data_flow: |
    Raw Data (archives) → GEO OS → geo_units.json → Application Systems

# === Processing Pipeline ===
pipeline:
  stages:
    - id: ingest
      name: Ingestion
      module: ingestion/normalize.py
      description: Normalize documents to Markdown
      inputs: [pdf, docx, md]
      outputs: [normalized_md]

    - id: chunk
      name: Chunking
      module: processing/chunk.py
      description: Split documents into fixed-size chunks
      inputs: [normalized_md]
      outputs: [chunks]

    - id: extract
      name: Extraction
      module: processing/extract.py
      description: Extract structure (headings, lists)
      inputs: [chunks]
      outputs: [structured_chunks]

    - id: export
      name: Export
      module: outputs/export_json.py
      description: Export to geo_units.json format
      inputs: [structured_chunks]
      outputs: [geo_units.json]

# === Skill Mapping ===
skills:
  # GEO OS uses Python modules as skills
  pipeline_skills:
    - normalize         # ingestion/normalize.py
    - chunk            # processing/chunk.py
    - extract          # processing/extract.py
    - export_json      # outputs/export_json.py

  # Future v0.2 skills (AI-powered)
  future:
    - vectorize        # Vector embedding generation
    - entity_extract   # Named entity recognition
    - claim_extract    # Claim extraction
    - graph_build      # Knowledge graph construction

# === Data Paths ===
paths:
  source_template: ~/data/archives/{source}      # Raw input data (read-only)
  processed_template: ~/data/processed/{source}  # Intermediate processing
  exports_template: ~/data/exports/{source}      # Per-source output
  merged_exports: ~/data/exports/_merged         # Merged output (all sources)

# === Truth Sources ===
# Reference: config/geo.yaml for full source definitions
sources:
  enabled:
    - geo_seo      # Priority 1: GEO-SEO Knowledge Base (46MB)
    - shengcai     # Priority 2: ShengCai Library (500MB+)
  disabled:
    - industry_reports
    - career_reports

# === Processing Configuration ===
processing:
  chunk_size: 600         # Characters per chunk
  chunk_overlap: 100      # Overlap between chunks
  max_heading_level: 3    # H1, H2, H3 extraction
  supported_formats:
    - pdf
    - docx
    - md
    - txt

# === Output Configuration ===
output:
  format: json
  version: "0.1.0"
  pretty_print: true
  create_latest_symlink: true
  filename_pattern: "geo_units_v{version}.json"

# === Evolution Configuration ===
evolution:
  enabled: false  # v0.1 is deterministic, no AI learning
  future_metrics:
    - extraction_accuracy
    - chunk_coherence
    - entity_precision

# === Quality Gates ===
quality:
  validation:
    - name: json_valid
      description: Output is valid JSON
    - name: schema_compliant
      description: Output matches geo_units schema
    - name: no_empty_chunks
      description: No chunks with empty content

# ----------------------------------------
# Truth Source Tier Guard
# ----------------------------------------
tier_guard:
  enabled: true

  rules:
    T0:
      allow_default_run: true
      allow_rag: true
      allow_export: true

    T1:
      allow_default_run: false
      allow_rag: true
      allow_export: true
      require_promotion_flag: true

    T2:
      allow_default_run: false
      allow_rag: false
      allow_export: false             # T2 forbidden from export (use refinement pipeline)
      require_refinement_pipeline: true

# ----------------------------------------
# T2 → T1 Refinement Pipeline
# ----------------------------------------
refinement:
  # Constitutional reference (MANDATORY)
  constitutional_reference: docs/architecture/T1_CANONICAL_DEFINITION.md

  # Pipeline stages
  stages:
    - id: candidate_extraction
      description: Extract T1 candidates from T2 raw content

    - id: truth_delta_gate
      description: Validate truth_delta field
      module: refinement/truth_delta_gate.py
      gate: TRUTH_DELTA_GATE

    - id: t1_promotion
      description: Promote validated units to T1

  # Gates (execution-time enforcement)
  gates:
    TRUTH_DELTA_GATE:
      module: refinement/truth_delta_gate.py
      class: TruthDeltaGate
      required_field: truth_delta
      validation_rules:
        - not_empty
        - not_vague
        - not_boilerplate
        - has_mechanism_or_causality
      on_failure: reject  # NO fallback
      auto_fill: false    # NO auto-fill

  # Rejection policy
  rejection_policy:
    action: retain_in_t2
    log: true
    log_path: ~/github/liye_os/_meta/logs/geo-os/refinement_rejections.log

  # T2 Raw input paths
  t2_raw_paths:
    template: ~/data/T2_raw/{source}
    sources:
      - sellersprite
      - junglescout
      - helium10
      - reddit_fba
